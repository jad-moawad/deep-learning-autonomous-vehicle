# Configuration for Transformer Planner
model:
  name: transformer_planner
  params:
    n_track: 10
    n_waypoints: 3
    d_model: 128
    nhead: 4
    num_decoder_layers: 3
    dropout: 0.1

data:
  dataset_path: data/drive_data
  batch_size: 128
  num_workers: 4

optimizer:
  type: adamw
  lr: 3e-4
  weight_decay: 1e-5

scheduler:
  type: cosine

training:
  epochs: 60
  grad_clip: 1.0
  lateral_weight: 2.0
  longitudinal_weight: 1.5
  early_stopping_patience: 10
  log_dir: logs
  checkpoint_dir: checkpoints

device: cuda